package org.ncgr.chatbot.openai;

import java.time.Duration;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import com.theokanning.openai.OpenAiHttpException;

import com.theokanning.openai.embedding.Embedding;
import com.theokanning.openai.embedding.EmbeddingRequest;
import com.theokanning.openai.embedding.EmbeddingResult;

import com.theokanning.openai.service.OpenAiService;

import com.theokanning.openai.completion.CompletionChoice;
import com.theokanning.openai.completion.CompletionRequest;
import com.theokanning.openai.completion.CompletionResult;

import com.theokanning.openai.completion.chat.ChatCompletionRequest;
import com.theokanning.openai.completion.chat.ChatCompletionResult;
import com.theokanning.openai.completion.chat.ChatMessage;

/**
 * Class holding an OpenAI service with convenience methods to perform OpenAI operations.
 */
public class OpenAi {

    // default parameters
    public static String EMBED_MODEL = "text-embedding-ada-002";
    public static String MODEL = "gpt-3.5-turbo-0301";
    public static int TIMEOUT_SECONDS = 120;
    public static double TEMPERATURE = 0.0;
    public static double FREQUENCY_PENALTY = 0.0;
    public static double PRESENCE_PENALTY = 0.0;
    public static String USER = "shokin@ncgr.org";

    OpenAiService service;

    /**
     * Construct by instantiating an OpenAiService with an API key.
     */
    public OpenAi(String apiKey, int timeoutSeconds) {
        this.service = new OpenAiService(apiKey, Duration.ofSeconds(timeoutSeconds));
    }

    /**
     * Return the instance OpenAI service.
     */
    public OpenAiService getService() {
        return service;
    }

    /**
     * Return a ChatCompletionRequest for a given prompt and input parameters.
     *
     * Input parameters:
     *
     * Double temperature;
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower
     * values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
     *
     * Double frequencyPenalty;
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far,
     * decreasing the model's likelihood to repeat the same line verbatim.
     *
     * Double presencePenalty;
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far,
     * increasing the model's likelihood to talk about new topics.
     *
     * List<ChatMessage> messages;
     * The messages to generate chat completions for, in the
     * <a href="https://platform.openai.com/docs/guides/chat/introduction">chat format</a>.
     *
     * Parameters set with defaults of this class:
     *
     * String model;
     * ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
     *
     * String user;
     * A unique identifier representing your end-user, which will help OpenAI to monitor and detect abuse.
     *
     * Parameters not in use:
     *
     * Integer maxTokens;
     * The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will
     * be (4096 - prompt tokens).
     *
     * Double topP;
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens
     * with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     * We generally recommend altering this ort emperature but not both.
     *
     * Integer n;
     * How many chat completion chatCompletionChoices to generate for each input message.
     *
     * Boolean stream;
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
     * <a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format">server-sent events</a>
     * as they become available, with the stream terminated by a data: [DONE] message.
     *
     * List<String> stop;
     * Up to 4 sequences where the API will stop generating further tokens.
     *
     * Map<String, Integer> logitBias;
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100
     * to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will
     * vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    public static ChatCompletionRequest getChatCompletionRequest(List<String> contexts, String query, double temperature, double frequencyPenalty, double presencePenalty) {
        // ChatMessages
        List<ChatMessage> messages = new ArrayList<>();
        // - system message instructs chat completion
        ChatMessage systemMessage = new ChatMessage();
        systemMessage.setRole("system");
        systemMessage.setContent("You are legumebot. You answer questions using only information provided in the assistant messages. " +
                                 "If the provided information is not sufficient to provide an answer, reply: 'The provided abstracts do not provide an answer to your question.'");
        messages.add(systemMessage);
        // - assistant messages contain contexts
        for (String context : contexts) {
            ChatMessage asstMessage = new ChatMessage();
            asstMessage.setRole("assistant");
            asstMessage.setContent(context);
            messages.add(asstMessage);
        }
        // - user message contains query
        ChatMessage userMessage = new ChatMessage();
        userMessage.setRole("user");
        userMessage.setContent("Respond to the following prompt based only on the content in the assistant chat messages:\n\n" +
                               query);
        messages.add(userMessage);
        // create request with model parameters
        ChatCompletionRequest request = new ChatCompletionRequest();
        request.setModel(MODEL);
        request.setUser(USER);
        request.setMessages(messages);
        request.setTemperature(temperature);
        request.setFrequencyPenalty(frequencyPenalty);
        request.setPresencePenalty(presencePenalty);
        return request;
    }
    
    /**
     * Get a ChatCompletionResult for a given ChatCompletionRequest.
     *
     * String id = Unique id assigned to this chat completion.
     * String object = The type of object returned, should be "chat.completion"
     * long created = The creation time in epoch seconds.
     * String model = The GPT-3.5 model used.
     * List<ChatCompletionChoice> choices = A list of all generated completions.
     * Usage usage = The API usage for this request.
     */
    public ChatCompletionResult getChatCompletionResult(ChatCompletionRequest request) throws OpenAiHttpException {
        return service.createChatCompletion(request);
    }

    /**
     * Retrieve an encoded query with embeddings.
     * requires: EMBED_MODEL
     *
     * @param query the query (question) being asked
     * @return a List of Float containing the encoded query  
     */
    public List<Float> getEncodedQuery(String query) {
        // create the embedding request
        EmbeddingRequest embeddingRequest = EmbeddingRequest.builder()
            .model(EMBED_MODEL)
            .input(Collections.singletonList(query))
            .build();
        // get the embeddings vector for the given question
        List<Embedding> embeddings = service.createEmbeddings(embeddingRequest).getData();
        List<Double> vector = embeddings.get(0).getEmbedding();
        // convert to List of Float
        List<Float> encodedQuery = new ArrayList<>();
        for (Double d : vector) {
            encodedQuery.add(d.floatValue());
        }
        return encodedQuery;
    }

    /**
     * Command-line utility.
     */
    public static void main(String[] args) {
        if (args.length<1) {
            System.err.println("OpenAi <query text>");
            System.exit(1);
        }
        String query = args[0];
        // the OpenAI embedding model to use
        String EMBED_MODEL = "text-embedding-ada-002";
        // get OpenAI API key from environment
        String apiKey = System.getenv().get("OPENAI_API_KEY");
        OpenAi openAi = new OpenAi(apiKey, TIMEOUT_SECONDS);
        System.out.println(openAi.getEncodedQuery(query));
    }
}
